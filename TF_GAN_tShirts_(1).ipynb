{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "metadata": {
      "interpreter": {
        "hash": "afa55092dc2bf38ba6aef7cffc57f3306e69ad8765fa36c252346589134cf799"
      }
    },
    "interpreter": {
      "hash": "c931ba401747e1100110d99c7b2e1195adf3961a7e00160e720e39c4d164b397"
    },
    "colab": {
      "name": "TF_GAN_tShirts (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# Check that imports for the rest of the file work.\r\n",
        "!pip install tensorflow\r\n",
        "!pip install tensorflow-gan\r\n",
        "!pip install tensorflow-datasets\r\n",
        "!pip install keras\r\n",
        "!pip install opencv-python\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "import tensorflow_gan as tfgan\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Allow matplotlib images to render immediately.\r\n",
        "%matplotlib inline\r\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)  # Disable noisy outputs.\r\n",
        "#evaluates operations immediately without building graphs\r\n",
        "\r\n",
        "tf.disable_v2_behavior()\r\n",
        "tf.compat.v1.enable_resource_variables()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\brad\\anaconda3\\lib\\site-packages (2.4.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (3.15.6)\n",
            "Requirement already satisfied: absl-py~=0.10 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: six~=1.15.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: gast==0.3.3 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (50.3.1.post20201107)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\brad\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: tensorflow-gan in c:\\users\\brad\\anaconda3\\lib\\site-packages (2.0.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.7 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-gan) (0.12.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-gan) (0.11.0)\n",
            "Requirement already satisfied: dm-tree in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (0.1.5)\n",
            "Requirement already satisfied: gast>=0.3.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (0.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (1.19.2)\n",
            "Requirement already satisfied: decorator in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (4.4.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-hub>=0.2->tensorflow-gan) (3.15.6)\n",
            "Requirement already satisfied: tensorflow-datasets in c:\\users\\brad\\anaconda3\\lib\\site-packages (4.2.0)\n",
            "Requirement already satisfied: future in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.18.2)\n",
            "Requirement already satisfied: dill in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (3.15.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.24.0)\n",
            "Requirement already satisfied: tensorflow-metadata in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.29.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (4.50.2)\n",
            "Requirement already satisfied: absl-py in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.12.0)\n",
            "Requirement already satisfied: promise in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (5.1.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.19.2)\n",
            "Requirement already satisfied: termcolor in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.1.0)\n",
            "Requirement already satisfied: six in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (20.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.6.20)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.53.0)\n",
            "Requirement already satisfied: keras in c:\\users\\brad\\anaconda3\\lib\\site-packages (2.4.3)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\brad\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
            "Requirement already satisfied: h5py in c:\\users\\brad\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from keras) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from keras) (1.19.2)\n",
            "Requirement already satisfied: six in c:\\users\\brad\\anaconda3\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
            "Requirement already satisfied: opencv-python in c:\\users\\brad\\anaconda3\\lib\\site-packages (4.5.1.48)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from opencv-python) (1.19.2)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic6yNYj5fJ3m",
        "outputId": "67e66fc0-7df2-44d1-fdc4-f5ae05c98b13"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# import tempfile\r\n",
        "# import tensorflow_hub as tfhub\r\n",
        "# tmpdir = tempfile.mkdtemp()\r\n",
        "\r\n",
        "# modGraph = tf.Graph()\r\n",
        "# with tf.Session(graph= tf.compat.v1.get_default_graph()) as session: \r\n",
        "#     mm = 'https://tfhub.dev/deepmind/ganeval-cifar10-convnet/1'\r\n",
        "#     module = tfhub.Module(mm)mport tempfile\r\n",
        "# import tensorflow_hub as tfhub\r\n",
        "# tmpdir = tempfile.mkdtemp()\r\n",
        "\r\n",
        "# modGraph = tf.Graph()\r\n",
        "# with tf.Session(graph= tf.compat.v1.get_default_graph()) as session: \r\n",
        "#     mm = 'https://tfhub.dev/deepmind/ganeval-cifar10-convnet/1'\r\n",
        "#     module = tfhub.Module(mm)"
      ],
      "outputs": [],
      "metadata": {
        "id": "L17zptgtfJ3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# from google.colab import drive\r\n",
        "# drive.mount('/content/gdrive')"
      ],
      "outputs": [],
      "metadata": {
        "id": "n7qH15u7xPsW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "# import os\r\n",
        "# from PIL import Image, ImageOps\r\n",
        "# import matplotlib.pyplot as plt\r\n",
        "# import numpy as np\r\n",
        "# import pathlib\r\n",
        "# shirtDir = os.listdir(\"SixDollarImages/\")\r\n",
        "\r\n",
        "# import tensorflow_datasets as tfds\r\n",
        "# import tensorflow.compat.v1 as tf\r\n",
        "# import gc\r\n",
        "# import cv2\r\n",
        "\r\n",
        "# g = tf.Graph()\r\n",
        "\r\n",
        "# # with tf.Session(graph=g) as session:\r\n",
        "# def input_fn(mode, params):\r\n",
        "\r\n",
        "#   shirtListTrain = []\r\n",
        "#   shirtListTest = []\r\n",
        "#   index = 0\r\n",
        "\r\n",
        "#   for file in shirtDir:\r\n",
        "#     im = Image.open(\"SixDollarImages/\" + file)\r\n",
        "#     im = im.resize((112, 112))\r\n",
        "#     # im = im.resize((32, 32))\r\n",
        "#     # im = im.resize((28, 28))\r\n",
        "#     im = im.convert(\"RGB\")\r\n",
        "\r\n",
        "#     im = np.array(im).reshape((112, 112, 3))\r\n",
        "#     # im = np.array(im).reshape((32, 32, 3))\r\n",
        "#     # im = np.array(im).reshape((28, 28, 1))\r\n",
        "#     # im = tf.convert_to_tensor(im, dtype=tf.uint8)\r\n",
        "#     im = tf.convert_to_tensor(im, dtype=tf.float32)\r\n",
        "\r\n",
        "#     if index < 20:\r\n",
        "#       shirtListTrain.append(im)\r\n",
        "#     else:\r\n",
        "#       shirtListTest.append(im)\r\n",
        "#     index = index + 1\r\n",
        "\r\n",
        "#   datasetTrain = tf.data.Dataset.from_tensor_slices(shirtListTrain)\r\n",
        "#   datasetTest = tf.data.Dataset.from_tensor_slices(shirtListTest)\r\n",
        "\r\n",
        "#   assert 'batch_size' in params\r\n",
        "#   assert 'noise_dims' in params\r\n",
        "#   bs = params['batch_size']\r\n",
        "#   nd = params['noise_dims']\r\n",
        "#   split = 'train' if mode == tf.estimator.ModeKeys.TRAIN else 'test'\r\n",
        "#   shuffle = (mode == tf.estimator.ModeKeys.TRAIN)\r\n",
        "#   just_noise = (mode == tf.estimator.ModeKeys.PREDICT)\r\n",
        "  \r\n",
        "#   #for each tensor call tf.random_normal\r\n",
        "#   #lambda is an unnamed python function \r\n",
        "#   # _ is used to ignore certian values, here we are ignoring the index\r\n",
        "#   noise_ds = (tf.data.Dataset.from_tensors(0).repeat()\r\n",
        "#               .map(lambda _: tf.random_normal([bs, nd])))\r\n",
        "  \r\n",
        "#   if just_noise:\r\n",
        "#     return noise_ds\r\n",
        "  \r\n",
        "#   #define everything as tensors\r\n",
        "#   def process_path(tensorInput):\r\n",
        "#     #{'image': <tf.Tensor 'args_0:0' shape=(28, 28, 1) dtype=uint8>, 'label': <tf.Tensor 'args_1:0' shape=() dtype=int64>}\r\n",
        "#     image = (tf.cast(tensorInput, tf.float32) - 127.5) / 127.5\r\n",
        "#     #Tensor(\"truediv:0\", shape=(28, 28, 1), dtype=float32)\r\n",
        "\r\n",
        "#     return image\r\n",
        "\r\n",
        "#   if split == 'train':\r\n",
        "#     #returns a map() object (which is an iterator)\r\n",
        "#     #stores data in a temporary location instead of retrieving from source everytime\r\n",
        "    \r\n",
        "#     images_ds = datasetTrain.map(process_path)\r\n",
        "      \r\n",
        "#   else:\r\n",
        "#     images_ds = datasetTest.map(process_path)\r\n",
        "\r\n",
        "#   if shuffle:\r\n",
        "#     images_ds = images_ds.shuffle(\r\n",
        "#         buffer_size=5, reshuffle_each_iteration=True)\r\n",
        "#   images_ds = (images_ds.batch(bs, drop_remainder=True)\r\n",
        "#               .prefetch(tf.data.experimental.AUTOTUNE))\r\n",
        "\r\n",
        "\r\n",
        "#   # <PrefetchDataset shapes: (5, 28, 28, 1), types: tf.float32>\r\n",
        "#   return tf.data.Dataset.zip((noise_ds, images_ds))\r\n",
        "\r\n",
        "\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "\r\n",
        "def input_fn(mode, params):\r\n",
        "  assert 'batch_size' in params\r\n",
        "  assert 'noise_dims' in params\r\n",
        "  bs = params['batch_size']\r\n",
        "  nd = params['noise_dims']\r\n",
        "  # split = 'train' if mode == tf.estimator.ModeKeys.TRAIN else 'test'\r\n",
        "  split = 'train' if mode == tf.estimator.ModeKeys.TRAIN else 'train'\r\n",
        "  shuffle = (mode == tf.estimator.ModeKeys.TRAIN)\r\n",
        "  just_noise = (mode == tf.estimator.ModeKeys.PREDICT)\r\n",
        "  \r\n",
        "  noise_ds = (tf.data.Dataset.from_tensors(0).repeat()\r\n",
        "              .map(lambda _: tf.random.normal([bs, nd])))\r\n",
        "  \r\n",
        "  if just_noise:\r\n",
        "    return noise_ds\r\n",
        "\r\n",
        "  def _preprocess(element):\r\n",
        "    # Map [0, 255] to [-1, 1].\r\n",
        "    images = (tf.cast(element['image'], tf.float32) - 127.5) / 127.5\r\n",
        "    return images\r\n",
        "  \r\n",
        "  images_ds = (tfds.load('cifar10', split=split)\r\n",
        "               .map(_preprocess)\r\n",
        "               .cache()\r\n",
        "               .repeat())\r\n",
        "  if shuffle:\r\n",
        "    images_ds = images_ds.shuffle(\r\n",
        "        buffer_size=10000, reshuffle_each_iteration=True)\r\n",
        "  images_ds = (images_ds.batch(bs, drop_remainder=True)\r\n",
        "               .prefetch(tf.data.experimental.AUTOTUNE))\r\n",
        "\r\n",
        "  print(images_ds)\r\n",
        "  return tf.data.Dataset.zip((noise_ds, images_ds))"
      ],
      "outputs": [],
      "metadata": {
        "id": "BEAVZdgjfJ3q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "# import tempfile\r\n",
        "# import tensorflow_hub as tfhub\r\n",
        "# tmpdir = tempfile.mkdtemp()\r\n",
        "\r\n",
        "# INCEPTION_MODULE = \"https://tfhub.dev/google/efficientnet/b0/classification/1\"\r\n",
        "# m = tf.keras.Sequential([tfhub.KerasLayer(INCEPTION_MODULE)])\r\n",
        "# # m = tfhub.load(INCEPTION_MODULE)\r\n",
        "# checkpoint = tf.train.Checkpoint(model=m)\r\n",
        "# # inceptionv3_saved_path = os.path.join(tmpdir, \"inceptionv3/1/\")\r\n",
        "# # tf.saved_model.save(pretrained_model, inceptionv3_saved_path)\r\n",
        "\r\n",
        "# # save_path = checkpoint.save('/tmp/bclass_checkpoint')\r\n",
        "# save_path = checkpoint.save('save/RestoreV2')"
      ],
      "outputs": [],
      "metadata": {
        "id": "PTwD9yUVfJ3r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "# import matplotlib.pyplot as plt\r\n",
        "# import tensorflow_datasets as tfds\r\n",
        "# import tensorflow_gan as tfgan\r\n",
        "# import numpy as np\r\n",
        "\r\n",
        "# params = {'batch_size': 100, 'noise_dims':64}\r\n",
        "# with tf.Graph().as_default():\r\n",
        "#   ds = input_fn(tf.estimator.ModeKeys.TRAIN, params)\r\n",
        "#   numpy_imgs = next(iter(tfds.as_numpy(ds)))[1]\r\n",
        "# img_grid = tfgan.eval.python_image_grid(numpy_imgs, grid_shape=(10, 10))\r\n",
        "# plt.axis('off')\r\n",
        "# plt.imshow(np.squeeze(img_grid))\r\n",
        "# plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "LMH_TfqRfJ3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "def _dense(inputs, units, l2_weight):\r\n",
        "  return tf.layers.dense(\r\n",
        "      inputs, units, None,\r\n",
        "      kernel_initializer=tf.keras.initializers.glorot_uniform,\r\n",
        "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\r\n",
        "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\r\n",
        "\r\n",
        "def _batch_norm(inputs, is_training):\r\n",
        "  return tf.layers.batch_normalization(\r\n",
        "      inputs, momentum=0.999, epsilon=0.001, training=is_training)\r\n",
        "\r\n",
        "def _deconv2d(inputs, filters, kernel_size, stride, l2_weight):\r\n",
        "  return tf.layers.conv2d_transpose(\r\n",
        "      inputs, filters, [kernel_size, kernel_size], strides=[stride, stride], \r\n",
        "      activation=tf.nn.relu, padding='same',\r\n",
        "      kernel_initializer=tf.keras.initializers.glorot_uniform,\r\n",
        "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\r\n",
        "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\r\n",
        "\r\n",
        "def _conv2d(inputs, filters, kernel_size, stride, l2_weight):\r\n",
        "  return tf.layers.conv2d(\r\n",
        "      inputs, filters, [kernel_size, kernel_size], strides=[stride, stride], \r\n",
        "      activation=None, padding='same',\r\n",
        "      kernel_initializer=tf.keras.initializers.glorot_uniform,\r\n",
        "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\r\n",
        "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\r\n",
        "    "
      ],
      "outputs": [],
      "metadata": {
        "id": "0WCkw5fafJ3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "def unconditional_generator(noise, mode, weight_decay=2.5e-5):\r\n",
        "  # \"\"\"Generator to produce unconditional MNIST images.\"\"\"\r\n",
        "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n",
        "  \r\n",
        "  sf = 16\r\n",
        "\r\n",
        "  net = _dense(noise, 4 * 4 * (sf * 8), weight_decay)\r\n",
        "  net = _batch_norm(net, is_training)\r\n",
        "  net = tf.reshape(net, [-1, 4, 4, (sf * 8)])\r\n",
        "\r\n",
        "  net = _deconv2d(net, (sf * 4), 5, 2, weight_decay)\r\n",
        "  net = tf.nn.relu(_batch_norm(net, is_training))\r\n",
        "\r\n",
        "  net = _deconv2d(net, (sf * 2), 5, 2, weight_decay)\r\n",
        "  net = tf.nn.relu(_batch_norm(net, is_training))\r\n",
        "\r\n",
        "  net = _deconv2d(net, (sf * 2), 5, 2, weight_decay)\r\n",
        "  net = tf.nn.relu(_batch_norm(net, is_training))\r\n",
        "\r\n",
        "  net = _deconv2d(net, (sf * 2), 5, 2, weight_decay)\r\n",
        "  net = tf.nn.relu(_batch_norm(net, is_training))\r\n",
        "\r\n",
        "  net = _conv2d(net, 3, 5, 2, 0.0)\r\n",
        "  net = tf.tanh(net)\r\n",
        "\r\n",
        "  return net"
      ],
      "outputs": [],
      "metadata": {
        "id": "zvhKEw1hfJ3t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "_leaky_relu = lambda net: tf.nn.leaky_relu(net, alpha=0.01)\r\n",
        "\r\n",
        "def unconditional_discriminator(img, unused_conditioning, mode, weight_decay=2.5e-5):\r\n",
        "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n",
        "  \r\n",
        "  sf = 16\r\n",
        "  net = _conv2d(img, sf, 5, 2, weight_decay)\r\n",
        "  net = _batch_norm(net, is_training)\r\n",
        "  net = _conv2d(img, sf, 5, 2, weight_decay)\r\n",
        "  net = _batch_norm(net, is_training)\r\n",
        "  net = _leaky_relu(net)\r\n",
        "\r\n",
        "  net = _conv2d(img, sf * 2, 5, 2, weight_decay)\r\n",
        "  net = _batch_norm(net, is_training)\r\n",
        "  net = _conv2d(img, sf * 2, 5, 2, weight_decay)\r\n",
        "  net = _batch_norm(net, is_training)\r\n",
        "  net = _leaky_relu(net)\r\n",
        "\r\n",
        "  net = _conv2d(img, sf * 4, 5, 2, weight_decay)\r\n",
        "  net = _batch_norm(net, is_training)\r\n",
        "  net = _conv2d(img, sf * 4, 5, 2, weight_decay)\r\n",
        "  net = _batch_norm(net, is_training)\r\n",
        "  net = _leaky_relu(net)\r\n",
        "\r\n",
        "  net = _conv2d(img, sf * 8, 5, 2, weight_decay)\r\n",
        "  net = _batch_norm(net, is_training)\r\n",
        "  net = _conv2d(img, sf * 8, 5, 2, weight_decay)\r\n",
        "  net = _batch_norm(net, is_training)\r\n",
        "  net = _leaky_relu(net)\r\n",
        "\r\n",
        "  net = tf.keras.layers.GlobalMaxPooling2D()(net)\r\n",
        "  net = _dense(net, 1, weight_decay)\r\n",
        "\r\n",
        "  return net"
      ],
      "outputs": [],
      "metadata": {
        "id": "EntNGSa2fJ3t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "# calculate inception score with Keras\r\n",
        "from math import floor\r\n",
        "from numpy import ones\r\n",
        "from numpy import expand_dims\r\n",
        "from numpy import log\r\n",
        "from numpy import mean\r\n",
        "from numpy import std\r\n",
        "from numpy import exp\r\n",
        "from keras.applications.inception_v3 import InceptionV3\r\n",
        "from keras.applications.inception_v3 import preprocess_input\r\n",
        "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\r\n",
        "\r\n",
        "from tensorflow_gan.examples.cifar import util\r\n",
        "\r\n",
        "import tempfile\r\n",
        "import tensorflow_hub as tfhub\r\n",
        "tmpdir = tempfile.mkdtemp()\r\n",
        "\r\n",
        "# INCEPTION_MODULE = \"https://tfhub.dev/google/imagenet/inception_v3/classification/5\"\r\n",
        "INCEPTION_MODULE = \"https://tfhub.dev/google/imagenet/inception_v3/classification/1\"\r\n",
        "MNIST_MODULE = 'https://tfhub.dev/tensorflow/tfgan/eval/mnist/logits/1'\r\n",
        "mm = 'https://tfhub.dev/deepmind/ganeval-cifar10-convnet/1'\r\n",
        "\r\n",
        "# def calculate_inception_score(images, n_split=1, eps=1E-16):\r\n",
        "#     inception_classifier_fn = tfhub.load(INCEPTION_MODULE)\r\n",
        "#     score = tfgan.eval.classifier_score(images, inception_classifier_fn, 1)\r\n",
        "#     score.shape.assert_is_compatible_with([])\r\n",
        "\r\n",
        "#     return score\r\n",
        "\r\n",
        "def calculate_frechet_distance(real_images, generated_images, num_batches=1):\r\n",
        "    # # # m = tf.keras.Sequential([tfhub.KerasLayer(INCEPTION_MODULE)])\r\n",
        "    # # # inception_classifier_fn = tfhub.load(INCEPTION_MODULE)\r\n",
        "    # module = tfhub.Module(mm)\r\n",
        "    # frechet_distance = tfgan.eval.frechet_classifier_distance(real_images, generated_images, module, num_batches)\r\n",
        "    frechet_distance = tfgan.eval.frechet_inception_distance(real_images, generated_images, num_batches=1)\r\n",
        "    frechet_distance.shape.assert_is_compatible_with([])\r\n",
        "\r\n",
        "    return frechet_distance\r\n",
        "\r\n",
        "def calculate_inception_score(images, n_split=1, eps=1E-16):\r\n",
        "    # score = util.get_inception_scores(images, 5, 5)\r\n",
        "    score = tfgan.eval.inception_score(images, num_batches=1)\r\n",
        "    return score\r\n",
        "    "
      ],
      "outputs": [],
      "metadata": {
        "id": "PAhwJkWFfJ3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "from tensorflow_gan.examples.mnist import util as eval_util\r\n",
        "import os\r\n",
        "\r\n",
        "def get_eval_metric_ops_fn(gan_model):\r\n",
        "  real_data_logits = tf.reduce_mean(gan_model.discriminator_real_outputs)\r\n",
        "  gen_data_logits = tf.reduce_mean(gan_model.discriminator_gen_outputs)\r\n",
        "  \r\n",
        "  print(\"incep score\")\r\n",
        "  real_mnist_score = calculate_inception_score(gan_model.real_data)\r\n",
        "  generated_mnist_score = calculate_inception_score(gan_model.generated_data)\r\n",
        "  \r\n",
        "  frechet_distance = calculate_frechet_distance(\r\n",
        "      gan_model.real_data, gan_model.generated_data)\r\n",
        "  return {\r\n",
        "      'real_data_logits': tf.metrics.mean(real_data_logits),\r\n",
        "      'gen_data_logits': tf.metrics.mean(gen_data_logits),\r\n",
        "      'real_mnist_score': tf.metrics.mean(real_mnist_score),\r\n",
        "      'mnist_score': tf.metrics.mean(generated_mnist_score),\r\n",
        "      'frechet_distance': tf.metrics.mean(frechet_distance),\r\n",
        "  }\r\n",
        "\r\n",
        "# def get_eval_metric_ops_fn(gan_model):\r\n",
        "#   real_data_logits = tf.reduce_mean(gan_model.discriminator_real_outputs)\r\n",
        "#   gen_data_logits = tf.reduce_mean(gan_model.discriminator_gen_outputs)\r\n",
        "#   real_mnist_score = eval_util.mnist_score(gan_model.real_data)\r\n",
        "#   generated_mnist_score = eval_util.mnist_score(gan_model.generated_data)\r\n",
        "#   frechet_distance = eval_util.mnist_frechet_distance(\r\n",
        "#       gan_model.real_data, gan_model.generated_data)\r\n",
        "#   return {\r\n",
        "#       'real_data_logits': tf.metrics.mean(real_data_logits),\r\n",
        "#       'gen_data_logits': tf.metrics.mean(gen_data_logits),\r\n",
        "#       'real_mnist_score': tf.metrics.mean(real_mnist_score),\r\n",
        "#       'mnist_score': tf.metrics.mean(generated_mnist_score),\r\n",
        "#       'frechet_distance': tf.metrics.mean(frechet_distance),\r\n",
        "#   }"
      ],
      "outputs": [],
      "metadata": {
        "id": "Yeac5pVEfJ3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "train_batch_size = 5 #@param\r\n",
        "noise_dimensions = 64 #@param\r\n",
        "# noise_dimensions = 128 #@param\r\n",
        "# generator_lr = 0.001 #@param\r\n",
        "generator_lr = 0.0002 #@param\r\n",
        "discriminator_lr = 0.0002 #@param\r\n",
        "\r\n",
        "def gen_opt():\r\n",
        "  gstep = tf.train.get_or_create_global_step()\r\n",
        "  base_lr = generator_lr\r\n",
        "  # Halve the learning rate at 1000 steps.\r\n",
        "  lr = tf.cond(gstep < 1000, lambda: base_lr, lambda: base_lr / 2.0)\r\n",
        "  return tf.train.AdamOptimizer(lr, 0.5)\r\n",
        "\r\n",
        "gan_estimator = tfgan.estimator.GANEstimator(\r\n",
        "    generator_fn=unconditional_generator,\r\n",
        "    discriminator_fn=unconditional_discriminator,\r\n",
        "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\r\n",
        "    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\r\n",
        "    params={'batch_size': train_batch_size, 'noise_dims': noise_dimensions},\r\n",
        "    generator_optimizer=gen_opt,\r\n",
        "    discriminator_optimizer=tf.train.AdamOptimizer(discriminator_lr, 0.5),\r\n",
        "    get_eval_metric_ops_fn=get_eval_metric_ops_fn)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Pyp-u6vdfJ3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "# Disable noisy output.\r\n",
        "tf.autograph.set_verbosity(0, False)\r\n",
        "\r\n",
        "#IMPORTNATN\r\n",
        "#what is the inception score for mnist?\r\n",
        "\r\n",
        "import time\r\n",
        "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\r\n",
        "steps_per_eval = 500 #@param\r\n",
        "max_train_steps = 20000 #@param\r\n",
        "batches_for_eval_metrics = 5 #@param|\r\n",
        "\r\n",
        "# Used to track metrics.\r\n",
        "steps = []\r\n",
        "real_logits, fake_logits = [], []\r\n",
        "real_mnist_scores, mnist_scores, frechet_distances = [], [], []\r\n",
        "\r\n",
        "cur_step = 0\r\n",
        "start_time = time.time()\r\n",
        "\r\n",
        "while cur_step < max_train_steps:\r\n",
        "  next_step = min(cur_step + steps_per_eval, max_train_steps)\r\n",
        "\r\n",
        "  start = time.time()\r\n",
        "  print(\"train\")\r\n",
        "  gan_estimator.train(input_fn, max_steps=next_step)\r\n",
        "  steps_taken = next_step - cur_step\r\n",
        "  time_taken = time.time() - start\r\n",
        "  print('Time since start: %.2f min' % ((time.time() - start_time) / 60.0))\r\n",
        "  print('Trained from step %i to %i in %.2f steps / sec' % (\r\n",
        "      cur_step, next_step, steps_taken / time_taken))\r\n",
        "  cur_step = next_step\r\n",
        "  \r\n",
        "  #save import model to check point\r\n",
        "  \r\n",
        "  # Calculate some metrics.\r\n",
        "  metrics = gan_estimator.evaluate(input_fn, steps=batches_for_eval_metrics)\r\n",
        "\r\n",
        "  steps.append(cur_step)\r\n",
        "  real_logits.append(metrics['real_data_logits'])\r\n",
        "  fake_logits.append(metrics['gen_data_logits'])\r\n",
        "  real_mnist_scores.append(metrics['real_mnist_score'])\r\n",
        "  mnist_scores.append(metrics['mnist_score'])\r\n",
        "  frechet_distances.append(metrics['frechet_distance'])\r\n",
        "  print('Average discriminator output on Real: %.2f  Fake: %.2f' % (\r\n",
        "      real_logits[-1], fake_logits[-1]))\r\n",
        "  print('Inception Score: %.2f / %.2f  Frechet Distance: %.2f' % (\r\n",
        "      mnist_scores[-1], real_mnist_scores[-1], frechet_distances[-1]))\r\n",
        "  \r\n",
        "  # Vizualize some images.\r\n",
        "  print(\"predict\")\r\n",
        "  iterator = gan_estimator.predict(\r\n",
        "      input_fn, hooks=[tf.train.StopAtStepHook(num_steps=21)])\r\n",
        "  try:\r\n",
        "    imgs = np.array([next(iterator) for _ in range(20)])\r\n",
        "  except StopIteration:\r\n",
        "    pass\r\n",
        "  tiled = tfgan.eval.python_image_grid(imgs, grid_shape=(2, 10))\r\n",
        "  plt.axis('off')\r\n",
        "  plt.imshow((np.squeeze(tiled) * 255).astype(np.uint8))\r\n",
        "  plt.show()\r\n",
        "  \r\n",
        "  \r\n",
        "# Plot the metrics vs step.\r\n",
        "plt.title('MNIST Frechet distance per step')\r\n",
        "plt.plot(steps, frechet_distances)\r\n",
        "plt.figure()\r\n",
        "plt.title('MNIST Score per step')\r\n",
        "plt.plot(steps, mnist_scores)\r\n",
        "plt.plot(steps, real_mnist_scores)\r\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "<PrefetchDataset shapes: (5, 32, 32, 3), types: tf.float32>\n",
            "Time since start: 2.15 min\n",
            "Trained from step 0 to 500 in 3.87 steps / sec\n",
            "<PrefetchDataset shapes: (5, 32, 32, 3), types: tf.float32>\n",
            "incep score\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Brad\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:329: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
            "  warnings.warn('`tf.layers.flatten` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average discriminator output on Real: 3.51  Fake: -0.33\n",
            "Inception Score: 1.00 / 3.17  Frechet Distance: 562.28\n",
            "predict\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"81.36pt\" version=\"1.1\" viewBox=\"0 0 349.2 81.36\" width=\"349.2pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-09-01T14:02:14.701662</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 81.36 \r\nL 349.2 81.36 \r\nL 349.2 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g clip-path=\"url(#p87cca550d4)\">\r\n    <image height=\"67\" id=\"image2409970a02\" transform=\"scale(1 -1)translate(0 -67)\" width=\"335\" x=\"7.2\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAAU8AAABDCAYAAAAceIc9AAAA/UlEQVR4nO3UsQ3AIADAMGDn/1M7tjeQBVWyL8iUOZ/9DgCOrNsBAH9kngCBeQIE5gkQmCdAYJ4AgXkCBOYJEJgnQGCeAIF5AgTmCRCYJ0BgngCBeQIE5gkQmCdAYJ4AgXkCBOYJEJgnQGCeAIF5AgTmCRCYJ0BgngCBeQIE5gkQmCdAYJ4AgXkCBOYJEJgnQGCeAIF5AgTmCRCYJ0BgngCBeQIE5gkQmCdAYJ4AgXkCBOYJEJgnQGCeAIF5AgTmCRCYJ0BgngCBeQIE5gkQmCdAYJ4AgXkCBOYJEJgnQGCeAIF5AgTmCRCYJ0BgngCBeQIE5gkQmCdAYJ4AwQfMRgKKUgsNjAAAAABJRU5ErkJggg==\" y=\"-7.16\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p87cca550d4\">\r\n   <rect height=\"66.96\" width=\"334.8\" x=\"7.2\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABRCAYAAABxPXV4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAABS0lEQVR4nO3aIRLCUAxAQX4HUcX9z4mqIj0BzGBeRXdtTNSbiKyZeQDQ2K5eAOBORBcgJLoAIdEFCIkuQEh0AULPX8PtePknA/jTZ3+vbzOXLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCa2au3gHgNly6ACHRBQiJLkBIdAFCogsQEl2A0AnHowqd3hfMdQAAAABJRU5ErkJggg=="
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "<PrefetchDataset shapes: (5, 32, 32, 3), types: tf.float32>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-28-efbceb198c99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m   \u001b[0mgan_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m   \u001b[0msteps_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_step\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcur_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m   \u001b[0mtime_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1173\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                                            self.config)\n\u001b[0;32m   1205\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[0m\u001b[0;32m   1207\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m                                              saving_listeners)\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[1;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[0;32m   1512\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    772\u001b[0m       \u001b[0mSame\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m     \"\"\"\n\u001b[1;32m--> 774\u001b[1;33m     return self._sess.run(\n\u001b[0m\u001b[0;32m    775\u001b[0m         \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1277\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1279\u001b[1;33m         return self._sess.run(\n\u001b[0m\u001b[0;32m   1280\u001b[0m             \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1281\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1367\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1370\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1432\u001b[1;33m     feed_dict = self._call_hook_before_run(run_context, actual_fetches,\n\u001b[0m\u001b[0;32m   1433\u001b[0m                                            feed_dict, options)\n\u001b[0;32m   1434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m_call_hook_before_run\u001b[1;34m(self, run_context, fetch_dict, user_feed_dict, options)\u001b[0m\n\u001b[0;32m   1458\u001b[0m     \u001b[0mhook_feeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1460\u001b[1;33m       \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1461\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrequest\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1462\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetches\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_gan\\python\\train.py\u001b[0m in \u001b[0;36mbefore_run\u001b[1;34m(self, run_context)\u001b[0m\n\u001b[0;32m   1104\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mbefore_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m       \u001b[0mrun_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_ops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1190\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1369\u001b[0m                            run_metadata)\n\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1373\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1360\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1449\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1450\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1451\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m                                             run_metadata)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UDm5FzlFfJ3v",
        "outputId": "a485ed29-4da8-49ed-e31f-0ccfaf0fb9d6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "OiSZiijyfJ3w"
      }
    }
  ]
}